{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import tqdm\r\n",
    "import sys\r\n",
    "import keras"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#dataset length\r\n",
    "LENGTH=10#in secs\r\n",
    "FEATURE_DIM=1582# opensmile feature dimension for an input chunk\r\n",
    "TIME_STAMPS=76\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# USER INPUT- features_dir\r\n",
    "curr=os.getcwd()\r\n",
    "repo_path=curr.split('/code')[0]\r\n",
    "data_path=repo_path+'/DIFv2'\r\n",
    "features_path=repo_path+'/features'\r\n",
    "features_dir=features_path+'/'+str(LENGTH)+'/open_chunks'\r\n",
    "saved_path=repo_path+'/saved_models'\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generator and data split code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import csv\r\n",
    "import numpy as np\r\n",
    "'''\r\n",
    "Input-  csv_file\r\n",
    "Output- partition train, val test. Each partition consists of list of .npy files and dictionary of labels.\r\n",
    "'''\r\n",
    "def train_test_split(csv_path):\r\n",
    "    label={'Drunk':1, 'Sober':0}\r\n",
    "    partition={}\r\n",
    "    train={}\r\n",
    "    val={}\r\n",
    "    test={}\r\n",
    "    \r\n",
    "    train_list=[]\r\n",
    "    val_list=[]\r\n",
    "    test_list=[]\r\n",
    "    train_label={}\r\n",
    "    val_label={}\r\n",
    "    test_label={}\r\n",
    "    \r\n",
    "    with open(csv_path) as csvfile:\r\n",
    "        reader=csv.reader(csvfile,delimiter=',')\r\n",
    "        for row in reader:\r\n",
    "            filename=row[2]\r\n",
    "            filename=filename[:-4]\r\n",
    "            if row[0]=='train':\r\n",
    "                train_label[filename]=label[row[1]]\r\n",
    "                train_list.append(filename)\r\n",
    "            elif row[0]=='val':\r\n",
    "                val_label[filename]=label[row[1]]\r\n",
    "                val_list.append(filename)\r\n",
    "            elif row[0]=='test':\r\n",
    "                test_label[filename]=label[row[1]]\r\n",
    "                test_list.append(filename)\r\n",
    "            else:\r\n",
    "                print(\"Error in label\")\r\n",
    "                return None\r\n",
    "    train['list']=train_list\r\n",
    "    val['list']=val_list\r\n",
    "    test['list']=test_list\r\n",
    "    \r\n",
    "    train['label']=train_label\r\n",
    "    val['label']=val_label\r\n",
    "    test['label']=test_label\r\n",
    "    \r\n",
    "    partition['train']=train\r\n",
    "    partition['val']=val\r\n",
    "    partition['test']=test\r\n",
    "    \r\n",
    "    return partition\r\n",
    "\r\n",
    "def count_classes(d):\r\n",
    "    values=list(d.values())\r\n",
    "    zeros=values.count(0)\r\n",
    "    return (zeros,len(values)-zeros)\r\n",
    "\r\n",
    "class DataGenerator(keras.utils.Sequence):\r\n",
    "    'Generates data for Keras'\r\n",
    "    def __init__(self, list_IDs, labels,datapath, batch_size=32, dim=(1582),n_classes=2, shuffle=True):\r\n",
    "        'Initialization'        \r\n",
    "        self.dim = dim\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.labels = labels\r\n",
    "        self.list_IDs = list_IDs\r\n",
    "        #self.n_channels = n_channels\r\n",
    "        self.n_classes = n_classes\r\n",
    "        self.shuffle = shuffle\r\n",
    "        self.on_epoch_end()\r\n",
    "        self.path=datapath\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        'Denotes the number of batches per epoch'\r\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\r\n",
    "\r\n",
    "\r\n",
    "    def on_epoch_end(self):\r\n",
    "        'Updates indexes after each epoch'\r\n",
    "        self.indexes = np.arange(len(self.list_IDs))\r\n",
    "        if self.shuffle == True:\r\n",
    "            np.random.shuffle(self.indexes)\r\n",
    "            \r\n",
    "    def __getitem__(self, index):\r\n",
    "        'Generate one batch of data'\r\n",
    "        # Generate indexes of the batch\r\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n",
    "\r\n",
    "        # Find list of IDs\r\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\r\n",
    "\r\n",
    "        # Generate data\r\n",
    "        X, y = self.__data_generation(list_IDs_temp)\r\n",
    "        import gc\r\n",
    "        gc.collect()\r\n",
    "        return X, y\r\n",
    "    \r\n",
    "    def __data_generation(self, list_IDs_temp):\r\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\r\n",
    "        # Initialization\r\n",
    "        X = np.empty((self.batch_size, *self.dim))\r\n",
    "        y = np.empty((self.batch_size), dtype=int)\r\n",
    "\r\n",
    "        # Generate data\r\n",
    "        for i, ID in enumerate(list_IDs_temp):\r\n",
    "            # Store sample\r\n",
    "            X[i,] = np.load(self.path+'/' + ID + '.npy')\r\n",
    "\r\n",
    "            # Store class\r\n",
    "            y[i] = self.labels[ID]\r\n",
    "\r\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build Model\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.1 Model creation and summary\n",
    "\n",
    "ordering of batch norm and dropout\n",
    "https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from keras.optimizers import Adam\r\n",
    "from keras.models import Model\r\n",
    "from keras.models import load_model\r\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation,BatchNormalization\r\n",
    "from keras.callbacks import ModelCheckpoint\r\n",
    "from keras.callbacks import TensorBoard\r\n",
    "from time import time\r\n",
    "import gc\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def create_model(num_class,lstm_units,dropout,input_shape=(None,1582)):\r\n",
    "    \"\"\"\r\n",
    "        Single layer LSTM\r\n",
    "    \"\"\"\r\n",
    "    X=Input(shape=input_shape)\r\n",
    "    norm=BatchNormalization()(X)\r\n",
    "    feat=LSTM(units=lstm_units)(norm)\r\n",
    "    drop=Dropout(rate=dropout)(feat)\r\n",
    "    prob=Dense(num_class, activation='sigmoid')(drop)\r\n",
    "    return Model(inputs = X, outputs = prob)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def create_model2(num_class,lstm_units1,lstm_units2,dropout,input_shape=(None,1582)):\r\n",
    "    \"\"\"\r\n",
    "        2 layer LSTM\r\n",
    "    \"\"\"\r\n",
    "    X=Input(shape=input_shape)\r\n",
    "    norm=BatchNormalization()(X)\r\n",
    "    feat=LSTM(units=lstm_units1, return_sequences=True)(norm)\r\n",
    "    drop=Dropout(rate=dropout)(feat)\r\n",
    "    feat2=LSTM(units=lstm_units2)(drop)\r\n",
    "    drop2=Dropout(rate=dropout)(feat2)\r\n",
    "    prob=Dense(num_class, activation='sigmoid')(drop2)\r\n",
    "    return Model(inputs = X, outputs = prob)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "lstm_units1=128\r\n",
    "lstm_units2=64\r\n",
    "dropout=.4\r\n",
    "hp=6\r\n",
    "model=create_model2(2,lstm_units1,lstm_units2,dropout,(TIME_STAMPS,FEATURE_DIM))\r\n",
    "model_path=repo_path+'/saved_models/open_chunks/'+str(LENGTH)+'/hp'+str(hp)#USER INPUT\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 76, 1582)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 76, 1582)          6328      \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 76, 128)           876032    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 76, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 931,898\n",
      "Trainable params: 928,734\n",
      "Non-trainable params: 3,164\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.2 Fit the model\n",
    "\n",
    "Class Imbalance \n",
    "https://groups.google.com/forum/#!topic/keras-users/MUO6v3kRHUw\n",
    "\n",
    "class_weight in keras\n",
    "https://keras.io/models/model/\n",
    "\n",
    "tensorboard \n",
    "https://fizzylogic.nl/2017/05/08/monitor-progress-of-your-keras-based-neural-network-using-tensorboard/\n",
    "\n",
    "save every k-epochs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "def load_keras_model(path):\r\n",
    "    if os.path.isfile(path):\r\n",
    "        return load_model(path)\r\n",
    "#Loading data filenames split\r\n",
    "split_path = '/media/netweb/2.0 TB/Vineet/repo/DIFv2/10/train_test_sets/1/split_4540_642_948.csv'## or enter path to the split.csv in the parent directory \r\n",
    "partition=train_test_split(split_path)\r\n",
    "print(\"Number of training examples \")\r\n",
    "print(len(partition['train']['list']))\r\n",
    "print(\"Number of validation examples \")\r\n",
    "print(len(partition['val']['list']))\r\n",
    "\r\n",
    "params = {'datapath':features_dir ,\r\n",
    "          'dim': (TIME_STAMPS,FEATURE_DIM),\r\n",
    "          'batch_size': 64,\r\n",
    "          'n_classes': 2,\r\n",
    "          'shuffle': True}\r\n",
    "    \r\n",
    "#weights for imbalance classes\r\n",
    "count=count_classes(partition['train']['label'])\r\n",
    "print(\"Class instances in training class.\\n Sober:\",count[0],\" Drunk:\",count[1])\r\n",
    "weight_0=float(count[0]+count[1])/float(count[0])\r\n",
    "weight_1=float(count[0]+count[1])/float(count[1])\r\n",
    "class_weight={0:weight_0, 1:weight_1}\r\n",
    "\r\n",
    "#instances in val set\r\n",
    "count=count_classes(partition['val']['label'])\r\n",
    "print(\"Class instances in val class.\\n Sober:\",count[0],\" Drunk:\",count[1])\r\n",
    "\r\n",
    "#instances in test set\r\n",
    "count=count_classes(partition['test']['label'])\r\n",
    "print(\"Class instances in test class.\\n Sober:\",count[0],\" Drunk:\",count[1])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of training examples \n",
      "4540\n",
      "Number of validation examples \n",
      "642\n",
      "Class instances in training class.\n",
      " Sober: 1045  Drunk: 3495\n",
      "Class instances in val class.\n",
      " Sober: 321  Drunk: 321\n",
      "Class instances in test class.\n",
      " Sober: 306  Drunk: 642\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\r\n",
    "\r\n",
    "#saving best model\r\n",
    "checkpoint = ModelCheckpoint(model_path+'/model-{epoch:03d}-{val_acc:03f}.h5', verbose=1, monitor='val_acc',save_best_only=False, mode='max',period=5)\r\n",
    "\r\n",
    "\r\n",
    "#tensorboard\r\n",
    "tensorboard = TensorBoard(log_dir=model_path+\"/log/{}\".format(time()))\r\n",
    "\r\n",
    "train_generator=DataGenerator(partition['train']['list'],partition['train']['label'], **params)\r\n",
    "val_generator=DataGenerator(partition['val']['list'],partition['val']['label'], **params)\r\n",
    "print(\"generator created\")\r\n",
    "model.fit_generator(generator=train_generator,epochs=50,validation_data=val_generator,\r\n",
    "                    use_multiprocessing=True,\r\n",
    "                    workers=6,callbacks=[checkpoint,tensorboard],class_weight=class_weight)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}