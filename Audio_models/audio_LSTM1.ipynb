{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import sys\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset length\n",
    "LENGTH=10#in secs\n",
    "FEATURE_DIM=1582# opensmile feature dimension for an input chunk\n",
    "TIME_STAMPS=76\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT- features_dir\n",
    "# features_dir- path to save/load opensmile chunk features\n",
    "curr=os.getcwd()\n",
    "repo_path=curr.split('/code')[0]\n",
    "features_path=repo_path+'/DIFv2'+'/features'\n",
    "features_dir=features_path+'/'+str(LENGTH)+'/open_chunks'\n",
    "saved_path=repo_path+'/saved_models'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator and data split code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "'''\n",
    "Input-  csv_file\n",
    "Output- partition train, val test. Each partition consists of list of .npy files and dictionary of labels.\n",
    "'''\n",
    "def train_test_split(csv_path):\n",
    "    label={'Drunk':1, 'Sober':0}\n",
    "    partition={}\n",
    "    train={}\n",
    "    val={}\n",
    "    test={}\n",
    "    \n",
    "    train_list=[]\n",
    "    val_list=[]\n",
    "    test_list=[]\n",
    "    train_label={}\n",
    "    val_label={}\n",
    "    test_label={}\n",
    "    \n",
    "    with open(csv_path) as csvfile:\n",
    "        reader=csv.reader(csvfile,delimiter=',')\n",
    "        for row in reader:\n",
    "            filename=row[2]\n",
    "            filename=filename[:-4]\n",
    "            if row[0]=='train':\n",
    "                train_label[filename]=label[row[1]]\n",
    "                train_list.append(filename)\n",
    "            elif row[0]=='val':\n",
    "                val_label[filename]=label[row[1]]\n",
    "                val_list.append(filename)\n",
    "            elif row[0]=='test':\n",
    "                test_label[filename]=label[row[1]]\n",
    "                test_list.append(filename)\n",
    "            else:\n",
    "                print(\"Error in label\")\n",
    "                return None\n",
    "    train['list']=train_list\n",
    "    val['list']=val_list\n",
    "    test['list']=test_list\n",
    "    \n",
    "    train['label']=train_label\n",
    "    val['label']=val_label\n",
    "    test['label']=test_label\n",
    "    \n",
    "    partition['train']=train\n",
    "    partition['val']=val\n",
    "    partition['test']=test\n",
    "    \n",
    "    return partition\n",
    "\n",
    "def count_classes(d):\n",
    "    values=list(d.values())\n",
    "    zeros=values.count(0)\n",
    "    return (zeros,len(values)-zeros)\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels,datapath, batch_size=32, dim=(1582),n_classes=2, shuffle=True):\n",
    "        'Initialization'        \n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        #self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.path=datapath\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        return X, y\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load(self.path+'/' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Model creation and summary\n",
    "\n",
    "ordering of batch norm and dropout\n",
    "https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation,BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_class,lstm_units,dropout,input_shape=(None,1582)):\n",
    "    \"\"\"\n",
    "        Single layer LSTM\n",
    "    \"\"\"\n",
    "    X=Input(shape=input_shape)\n",
    "    norm=BatchNormalization()(X)\n",
    "    feat=LSTM(units=lstm_units)(norm)\n",
    "    drop=Dropout(rate=dropout)(feat)\n",
    "    prob=Dense(num_class, activation='sigmoid')(drop)\n",
    "    return Model(inputs = X, outputs = prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2(num_class,lstm_units1,lstm_units2,dropout,input_shape=(None,1582)):\n",
    "    \"\"\"\n",
    "        2 layer LSTM\n",
    "    \"\"\"\n",
    "    X=Input(shape=input_shape)\n",
    "    norm=BatchNormalization()(X)\n",
    "    feat=LSTM(units=lstm_units1, return_sequences=True)(norm)\n",
    "    drop=Dropout(rate=dropout)(feat)\n",
    "    feat2=LSTM(units=lstm_units2)(drop)\n",
    "    drop2=Dropout(rate=dropout)(feat2)\n",
    "    prob=Dense(num_class, activation='sigmoid')(drop2)\n",
    "    return Model(inputs = X, outputs = prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 76, 1582)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 76, 1582)          6328      \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 76, 128)           876032    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 76, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 931,898\n",
      "Trainable params: 928,734\n",
      "Non-trainable params: 3,164\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_units1=128\n",
    "lstm_units2=64\n",
    "dropout=.4\n",
    "hp=6\n",
    "model=create_model2(2,lstm_units1,lstm_units2,dropout,(TIME_STAMPS,FEATURE_DIM))\n",
    "model_path=repo_path+'/saved_models/open_chunks/'+str(LENGTH)+'/hp'+str(hp)#USER INPUT, path to save/load model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Fit the model\n",
    "\n",
    "Class Imbalance \n",
    "https://groups.google.com/forum/#!topic/keras-users/MUO6v3kRHUw\n",
    "\n",
    "class_weight in keras\n",
    "https://keras.io/models/model/\n",
    "\n",
    "tensorboard \n",
    "https://fizzylogic.nl/2017/05/08/monitor-progress-of-your-keras-based-neural-network-using-tensorboard/\n",
    "\n",
    "save every k-epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples \n",
      "4540\n",
      "Number of validation examples \n",
      "642\n",
      "Class instances in training class.\n",
      " Sober: 1045  Drunk: 3495\n",
      "Class instances in val class.\n",
      " Sober: 321  Drunk: 321\n",
      "Class instances in test class.\n",
      " Sober: 306  Drunk: 642\n"
     ]
    }
   ],
   "source": [
    "def load_keras_model(path):\n",
    "    if os.path.isfile(path):\n",
    "        return load_model(path)\n",
    "#Loading data filenames split\n",
    "split_path = '/media/netweb/2.0 TB/Vineet/repo/DIFv2/10/train_test_sets/1/split_4540_642_948.csv'## or enter path to the split.csv in the parent directory \n",
    "partition=train_test_split(split_path)\n",
    "print(\"Number of training examples \")\n",
    "print(len(partition['train']['list']))\n",
    "print(\"Number of validation examples \")\n",
    "print(len(partition['val']['list']))\n",
    "\n",
    "params = {'datapath':features_dir ,\n",
    "          'dim': (TIME_STAMPS,FEATURE_DIM),\n",
    "          'batch_size': 64,\n",
    "          'n_classes': 2,\n",
    "          'shuffle': True}\n",
    "    \n",
    "#weights for imbalance classes\n",
    "count=count_classes(partition['train']['label'])\n",
    "print(\"Class instances in training class.\\n Sober:\",count[0],\" Drunk:\",count[1])\n",
    "weight_0=float(count[0]+count[1])/float(count[0])\n",
    "weight_1=float(count[0]+count[1])/float(count[1])\n",
    "class_weight={0:weight_0, 1:weight_1}\n",
    "\n",
    "#instances in val set\n",
    "count=count_classes(partition['val']['label'])\n",
    "print(\"Class instances in val class.\\n Sober:\",count[0],\" Drunk:\",count[1])\n",
    "\n",
    "#instances in test set\n",
    "count=count_classes(partition['test']['label'])\n",
    "print(\"Class instances in test class.\\n Sober:\",count[0],\" Drunk:\",count[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "\n",
    "#saving best model\n",
    "checkpoint = ModelCheckpoint(model_path+'/model-{epoch:03d}-{val_acc:03f}.h5', verbose=1, monitor='val_acc',save_best_only=False, mode='max',period=5)\n",
    "\n",
    "\n",
    "#tensorboard\n",
    "tensorboard = TensorBoard(log_dir=model_path+\"/log/{}\".format(time()))\n",
    "\n",
    "train_generator=DataGenerator(partition['train']['list'],partition['train']['label'], **params)\n",
    "val_generator=DataGenerator(partition['val']['list'],partition['val']['label'], **params)\n",
    "print(\"generator created\")\n",
    "model.fit_generator(generator=train_generator,epochs=50,validation_data=val_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=6,callbacks=[checkpoint,tensorboard],class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
